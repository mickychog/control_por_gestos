# ğŸ–ï¸ Control por Gestos Basado en VisiÃ³n Artificial

## DescripciÃ³n del Proyecto

El proyecto "Control por Gestos Basado en VisiÃ³n Artificial" tiene como objetivo desarrollar un sistema que permita a los usuarios interactuar con dispositivos mediante gestos de la mano, reemplazando los mÃ©todos tradicionales de entrada como el teclado y el ratÃ³n. Este sistema utiliza tÃ©cnicas avanzadas de visiÃ³n artificial y aprendizaje profundo para detectar y clasificar gestos en tiempo real, proporcionando una alternativa intuitiva y accesible para la interacciÃ³n humano-computadora.

## Estudiante

- **Nombre:** Miguel Angel Choque Garcia
- **Carrera:** IngenierÃ­a en Ciencias de la ComputaciÃ³n
- **Universidad:** Universidad Mayor Real y Pontificia San Francisco Xavier de Chuquisaca
- **Materia:** SIS330 Desarrollo de Aplicaciones Inteligentes
- **Semestre:** Septimo
- **Docente:** Ing. Walter Pacheco

## ğŸ“¬ Contacto

ğŸ“Œ EncuÃ©ntrame en:

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/miguel-choque-garcia/)

[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/mickychog)

ğŸ“§ **Correo:** [choque.garcia.miguelangel@usfx.bo](mailto:choque.garcia.miguelangel@usfx.bo)

## ğŸ“š ArtÃ­culo CientÃ­fico Final

ğŸ“„ El documento completo del proyecto se encuentra en la carpeta `Documents/Control-por-Gestos-Basado-en-Vision-Artificial.pdf`.

Este artÃ­culo cientÃ­fico presenta:

- Un **resumen ejecutivo** del sistema de control por gestos.
- Una descripciÃ³n detallada de la **metodologÃ­a**, basada en tÃ©cnicas de visiÃ³n artificial y aprendizaje profundo.
- Los **resultados obtenidos**, incluyendo una precisiÃ³n superior al 95% en la clasificaciÃ³n de gestos.
- Las **conclusiones y recomendaciones** para futuras mejoras del sistema.

ğŸ“¥ [Descargar documento del proyecto ](Documents/Control-por-Gestos-Basado-en-Vision-Artificial.pdf)

## Â¿CÃ³mo Funciona el Proyecto? ğŸ¤”

Â¡Bienvenido a la secciÃ³n donde te explicamos cÃ³mo funciona este emocionante proyecto de Control por Gestos Basado en VisiÃ³n Artificial! ğŸŒŸ AquÃ­ te cuento de una manera sencilla para que puedas entenderlo.

### 1. Captura de Video ğŸ“¹

Todo comienza cuando enciendes la cÃ¡mara web de tu computadora. ğŸ“¸ Nuestra aplicaciÃ³n captura el video en tiempo real, lo que significa que ve todo lo que la cÃ¡mara ve, Â¡justo en el momento en que ocurre!

<!-- ![Captura de Video](images/captura_video.png) -->

### 2. DetecciÃ³n de Manos ğŸ–ï¸

Una vez que la cÃ¡mara estÃ¡ encendida, nuestro sistema utiliza una tecnologÃ­a llamada MediaPipe Hands para detectar tus manos en el video. ğŸ¤² MediaPipe es como un mago que puede encontrar y seguir tus manos en cada cuadro del video, identificando puntos clave en tus dedos y palmas.

<!-- ![DetecciÃ³n de Manos](images/deteccion_manos.png) -->

### 3. ClasificaciÃ³n de Gestos âœ‹

AquÃ­ es donde entra en juego la inteligencia artificial. ğŸ§  Utilizamos un modelo llamado Vision Transformer (ViT), que es como un cerebro entrenado para reconocer diferentes gestos de la mano. ğŸ¤Ÿ Este modelo ha sido entrenado con miles de imÃ¡genes de gestos, por lo que puede decirte quÃ© gesto estÃ¡s haciendo en tiempo real.

Por ejemplo, si levantas tu dedo Ã­ndice, el modelo puede reconocer que estÃ¡s haciendo el gesto de "seÃ±alar". ğŸ‘† Si levantas dos dedos, puede reconocer el gesto de "paz". âœŒï¸

<!-- ![ClasificaciÃ³n de Gestos](images/clasificacion_gestos.png) -->

### 4. Mapeo de Gestos a Acciones ğŸ–±ï¸

Una vez que el modelo ha clasificado el gesto, nuestro sistema traduce ese gesto en una acciÃ³n especÃ­fica en tu computadora. ğŸ’» Por ejemplo:

- **Movimiento del Cursor:** Si mueves tu dedo Ã­ndice, el cursor en la pantalla se moverÃ¡ en la misma direcciÃ³n. ğŸ–±ï¸
- **Clic Izquierdo:** Si haces el gesto de "paz" (dedo Ã­ndice y medio levantados), el sistema realizarÃ¡ un clic izquierdo. âœŒï¸
- **Clic Derecho:** Si levantas tres dedos, el sistema realizarÃ¡ un clic derecho. ğŸ‘Œ
- **Doble Clic:** Si haces dos dedos juntos (indice y medio), el sistema realizarÃ¡ un doble clic. ğŸ‘Š
- **Ajustar brillo-Volumen:** Si haces el gesto de "ok" (dedo Ã­ndice y pulgar formando un cÃ­rculo), podrÃ¡s ajustar el brillo y volumen del sistema. ğŸ‘Œ
- **Scroll:** Si levantas cuatro dedos y mueves tu dedo Ã­ndice hacia arriba o abajo, podrÃ¡s desplazarte por la pantalla. ğŸ‘†ğŸ‘‡

<!-- ![Mapeo de Gestos](images/mapeo_gestos.png) -->

### 5. InteracciÃ³n en Tiempo Real â±ï¸

Todo este proceso ocurre en tiempo real, lo que significa que no hay retrasos perceptibles entre el momento en que haces un gesto y el momento en que la acciÃ³n se realiza en la pantalla. âš¡ Esto hace que la experiencia sea fluida y natural, como si estuvieras usando un ratÃ³n invisible. ğŸ–±ï¸ğŸ‘»

Â¡Y eso es todo! ğŸ‰ Ahora ya sabes cÃ³mo funciona este proyecto de Control por Gestos Basado en VisiÃ³n Artificial. Espero que te haya gustado y que te animes a probarlo. Â¡DiviÃ©rtete interactuando con tu computadora de una manera completamente nueva! ğŸš€

<img src="https://github.com/mickychog/control_por_gestos/blob/main/Images/ejemplo.png" alt="Ejemplo" width="700" height="450"/>

## ğŸ› ï¸ TecnologÃ­as Utilizadas:

- **Python**
- **Vision Transformer (ViT)** - Modelo de clasificaciÃ³n de imÃ¡genes
- **MediaPipe Hands** - DetecciÃ³n de puntos clave en las manos
- **OpenCV** - Captura y procesamiento de video
- **PyAutoGUI** - AutomatizaciÃ³n de acciones del sistema operativo
- **PyTorch & Transformers** - Entrenamiento e inferencia del modelo

## InstalaciÃ³n

1. **Clonar el Repositorio ğŸ“‚:**
   Primero, necesitas clonar el repositorio del proyecto desde GitHub. Abre tu terminal o lÃ­nea de comandos y ejecuta:

   ```bash
   git clone https://github.com/mickychog/control_por_gestos.git
   cd control_por_gestos
   ```

2. **Crear un Entorno Virtual ğŸŒ:**
   Es una buena prÃ¡ctica crear un entorno virtual para manejar las dependencias del proyecto. Ejecuta el siguiente comando para crear un entorno virtual llamado gest:

   ```bash
   python -m venv gest
   ```

3. **Activar el Entorno Virtual ğŸ”Œ:**
   Ahora, activa el entorno virtual. El comando depende de tu sistema operativo:

   ```bash
   gest\Scripts\activate
   ```

4. **Instalar las Dependencias ğŸ“¦:**
   Con el entorno virtual activo, instala las dependencias necesarias utilizando pip. AsegÃºrate de tener un archivo requirements.txt en el directorio del proyecto. Ejecuta el siguiente comando:

   ```bash
   pip install -r requirements.txt
   ```

5. **Descargar el Modelo ğŸ¤–:**
   NecesitarÃ¡s descargar el modelo preentrenado para que el sistema funcione correctamente. Descarga el modelo desde el siguiente enlace y colÃ³calo en el directorio models/ dentro del proyecto.
   [Modelo Preentrenado (model.safetensors)](https://drive.google.com/file/d/1sBTnkokPKcaYf9HjFDjU3fdn64dDCEEl/view?usp=sharing)

6. **Ejecutar el Proyecto ğŸƒ**
   Finalmente, puedes ejecutar el proyecto principal. AsegÃºrate de estar en el directorio del proyecto y ejecuta el archivo principal del proyecto.En este caso el archivo principal se llama gui.py, usa el siguiente comando:

   ```bash
   python gui.py
   ```

7. **SoluciÃ³n de Problemas ğŸ› ï¸**
   Si encuentras algÃºn problema durante la instalaciÃ³n, aquÃ­ tienes algunos consejos:

- Error al activar el entorno virtual: AsegÃºrate de que estÃ¡s utilizando la versiÃ³n correcta de Python y que el entorno virtual se creÃ³ correctamente.

- Problemas con las dependencias: Verifica que el archivo requirements.txt estÃ© completo y que todas las dependencias se instalen sin errores.

- Error al descargar el modelo: AsegÃºrate de tener una conexiÃ³n a internet estable y de que el enlace de descarga sea correcto.

## Estructura del proyecto

```bash
control_por_gestos/
â”œâ”€â”€ ğŸ“ config/
â”‚ â””â”€â”€ settings.py
â”‚
â”œâ”€â”€ ğŸ“ models/
â”‚ â””â”€â”€ model.safetensors
â”‚
â”œâ”€â”€ ğŸ“ src/
â”‚ â”œâ”€â”€ ğŸ“ enums/
â”‚ â”‚  â””â”€â”€ gesture_enums.py
â”‚ â”œâ”€â”€ gesture_controller.py
â”‚ â”œâ”€â”€ gesture_handlers.py
â”‚ â”œâ”€â”€ hand_recognition.py
â”‚
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ gui.py
```

### Archivos Clave

- gui.py:

  - FunciÃ³n: Interfaz de Usuario (Frontend).
  - DescripciÃ³n: Muestra la ventana principal de la aplicaciÃ³n, con controles para iniciar/detener la detecciÃ³n y una consola para ver los gestos reconocidos.

- gesture_controller.py:

  - FunciÃ³n: Controlador Principal (Backend de VisiÃ³n).
  - DescripciÃ³n: Gestiona la cÃ¡mara, procesa fotogramas con MediaPipe Hands, y pasa los datos de la mano a hand_recognition.py para identificar gestos. Muestra la vista de la cÃ¡mara en una pequeÃ±a ventana flotante y envÃ­a los gestos a gesture_handlers.py para la ejecuciÃ³n de acciones. Se comunica con gui.py a travÃ©s de una cola de mensajes.

- hand_recognition.py:

  - FunciÃ³n: Reconocimiento de Gestos (IA/LÃ³gica de ML).
  - DescripciÃ³n: Recibe los puntos de referencia de la mano y utiliza el modelo model.safetensors (o lÃ³gica basada en reglas) para clasificar el tipo de gesto realizado (ej. FIST, PEACE, OK).

- gesture_handlers.py:

  - FunciÃ³n: Ejecutor de Acciones (AutomatizaciÃ³n).
  - DescripciÃ³n: Contiene la lÃ³gica para traducir los gestos reconocidos en acciones del sistema (movimiento del ratÃ³n, clics, control de volumen/brillo, scroll) utilizando librerÃ­as como pyautogui, pycaw y screen_brightness_control. Implementa un sistema de banderas para asegurar que las acciones se disparen correctamente.

- enums/gesture_enums.py:

  - FunciÃ³n: Definiciones (Constantes).
  - DescripciÃ³n: Define enumeraciones para los gestos (Gest) y las etiquetas de las manos (HLabel), lo que mejora la legibilidad y estandarizaciÃ³n del cÃ³digo.

- config/settings.py:

  - FunciÃ³n: ConfiguraciÃ³n (ParÃ¡metros).
  - DescripciÃ³n: Almacena parÃ¡metros configurables del sistema, como el Ã­ndice de la cÃ¡mara, umbrales de confianza para MediaPipe y el nombre de la ventana de visualizaciÃ³n de la cÃ¡mara.

- models/model.safetensors:

  - FunciÃ³n: Modelo de Machine Learning.
  - DescripciÃ³n: Archivo que contiene el modelo entrenado para el reconocimiento de gestos, utilizado por hand_recognition.py para clasificar los gestos con mayor precisiÃ³n.

## âœ‹ Gestos Implementados

- ONE (Dedo Ã­ndice extendido): Mueve el cursor del ratÃ³n.
- PEACE (Dedos Ã­ndice y medio extendidos): Click izquierdo del ratÃ³n.
- THREE (Tres dedos extendidos): Click derecho del ratÃ³n.
- TWO_UP (Dedos Ã­ndice y pulgar extendidos, los demÃ¡s doblados): Doble clic izquierdo del ratÃ³n.
- FOUR (Cuatro dedos extendidos): Activa el modo de scroll. Mueve la mano vertical u horizontalmente para desplazar la pÃ¡gina.
- OK (Pulgar e Ã­ndice en contacto, los demÃ¡s extendidos): Activa el control de brillo/volumen. Mueve la mano verticalmente para ajustar el volumen y horizontalmente para el brillo.
- FIST (PuÃ±o cerrado): Sin acciÃ³n asignada (neutral).
- PALM (Mano abierta): Sin acciÃ³n asignada (neutral).
- CALL (Pulgar y meÃ±ique extendidos, los demÃ¡s doblados): Sin acciÃ³n asignada (neutral).
  <img src="Images/gestures.jpg" alt="Gestos" width="900" height="350"/>

## ğŸ¥ DemostraciÃ³n

<img src="Images/demo.gif" alt="Demostracion" width="720" height="400"/>

## Â© Declaratoria de AutorÃ­a

Este trabajo es de autorÃ­a exclusiva de **Miguel Choque GarcÃ­a**, estudiante de IngenierÃ­a en Ciencias de la ComputaciÃ³n de la **Universidad Mayor Real y Pontificia San Francisco Xavier de Chuquisaca**.

El proyecto **"Control por Gestos Basado en VisiÃ³n Artificial"** fue desarrollado durante el aÃ±o 2025 como parte del curso **SIS330 - Desarrollo de Aplicaciones Inteligentes**, bajo la orientaciÃ³n del docente **Ing. Walter Pacheco**.

Queda prohibida su reproducciÃ³n total o parcial sin el consentimiento explÃ­cito del autor. Cualquier uso acadÃ©mico debe reconocer debidamente al autor y a la instituciÃ³n.

> _Este proyecto tiene fines acadÃ©micos. Puede ser utilizado con fines de estudio, investigaciÃ³n y enseÃ±anza siempre que se cite correctamente._

## ğŸ“œ Licencia

Este proyecto estÃ¡ bajo la [Licencia MIT](LICENSE).  
Puedes usarlo, modificarlo y distribuirlo libremente, siempre que incluyas la debida atribuciÃ³n al autor original.
